[
  {
    "path": "posts/2021-04-25-sam-data-exploration/",
    "title": "SAM Data Exploration",
    "description": "Using alluvial plots and chi-squared tests to see whether the academic division of a specific Student Academic Mentor (SAM) impacts the type of students that reach out to that SAM.",
    "author": [
      {
        "name": "Tolga Atabas",
        "url": {}
      }
    ],
    "date": "2021-04-25",
    "categories": [],
    "contents": "\nGoals of the Project\nAlluvial plots are by far the most interesting plots I have seen. They are relatively non-traditional, in that it rarely resembles the usual cartesian plots that we see everyday. Personally, I think it is a great escape from the table and charts that bombard our screen everyday, especially in times of the pandemic.\nIn this project, I am using data collected by the Swarthmore Student Academic Mentor (SAM) program. The goal of the program is to look at the relationship between Swarthmore SAMs and their interactions with students. Furthermore, one possible question that the alluvial plot can help answer is whether the academic division of a specific SAM impacts the type of students that reach out to that SAM. For example, does a SAM who studies in the Social Sciences (SS) have more students from Social Sciences reaching out to them in comparison to Natural Sciences & Engineering (NSE) or Humanities (HU)? We will soon find out!\nTo set some background, the Swarthmore SAM program consists of a cohort of around 20 student peer leaders that serve as a central points of contact for Swarthmore students. SAMs are trained to provide advice in topics ranging from campus resources, time management, communication, course advising, and much more. Essentially, SAMs are a convenient way for students in the larger Swarthmore community to access the various resources offered by the campus.\nLibraries\nBefore we begin, the libraries used in this project are as follows:\ndplyr\ntidyverse\nggplot2\nggalluvial\nggthemes\nDT\nData\nSAM Interactions\nThe Swarthmore SAM Program started collecting more detailed data in regards to how SAMs interact with students. Each entry in the data is a single interaction between a student and a SAM.\n\n                                         Timestamp \n                                       \"character\" \n         How did you communicate with the student? \n                                       \"character\" \n                               Date of Interaction \n                                       \"character\" \n                       Day of week for interaction \n                                       \"character\" \n                                       Time of day \n                                       \"character\" \n                                               SAM \n                                       \"character\" \n                         Class year of the student \n                                       \"character\" \n                                           Student \n                                       \"character\" \n                          Interaction with Student \n                                       \"character\" \n                      Student's Academic Interests \n                                       \"character\" \nWhat types of questions did you answer or discuss? \n                                       \"character\" \n\nThe short summary above shows that each interaction records the time, date, SAM, and student information such as academic division and year.\nSAMs\nThis is a dataset that was copied from the Swarthmore SAMs website. This includes information about the SAMs and their academics interests. This will be important in studying the relationships between SAMs and students with regard to their academic divisions.\n\n       Name        Year       Major    Division \n\"character\"   \"numeric\" \"character\" \"character\" \n\nThe SAM Interactions and SAMs datasets will be joined together simply by using sam_interactions <- left_join(sam_interactions, sams, by = c(\"SAM\" = \"Name\"))\nBuilding the Alluvial\nData Preparation\nWe can think of an alluvial plot as having two axes, a source and a destination. The source in this case are the students, and their destinations that SAMs. We want to group students by division, and see how these students go to the SAMs for advice. Following this model, it is necessary to have the data structured in a way that we have the frequency or count of students in divison X that go to SAMs in division Y. This requires some restructuring of the data.\nSince the divisions of students are multi-selection inputs, the “Student’s Academic Interest” field can be filled in with more than one division. The same applies for SAMs. It is not uncommon for students at Swarthmore to have one major in the NSE division and another major/minor in the SS division, or some other combination of divisions. Since this is the case, I thought it was best to create a y/n field for each student division and SAM division. So I added a total of 7 more fields to the data set: student_nse student_ss student_hu student_un sam_nse sam_ss and sam_hu. There are four fields for students, “nse” for Natural Sciences & Engineering, “ss” for Social Sciences, “hu” for Humanities, and “un” for Undecided, each indicated as y or n. Similarly, “sam_div” is y or n if a SAM is studying in a particular division or not.\n\n\n#joining the data \nsam_interactions <- left_join(sam_interactions, sams, by = c(\"SAM\" = \"Name\"))\n\n#renaming columns for easier manipulation\ncolnames(sam_interactions) <- c(\"timestamp\",\"communcation_method\", \"date\", \"day\", \"time\", \"sam\", \"student_class_year\", \"student_status\",\"interaction\", \"student_division\",\"student_question\",\"sam_year\", \"sam_major\",\"sam_division\")\n\n#creating the additional y/n fields\nsam_interactions<-sam_interactions %>%\n  mutate(student_nse = if_else(grepl(\"Natural Sciences\", student_division, fixed=TRUE), \"y\", \"n\")) %>%\n  mutate(student_ss = if_else(grepl(\"Social Sciences\", student_division, fixed=TRUE), \"y\", \"n\")) %>%\n  mutate(student_hu = if_else(grepl(\"Humanities\", student_division, fixed=TRUE), \"y\", \"n\")) %>%\n  mutate(student_un = if_else(grepl(\"Undecided\", student_division, fixed=TRUE), \"y\", \"n\")) %>%\n  mutate(sam_nse = if_else(grepl(\"Natural Sciences\", sam_division, fixed=TRUE), \"y\", \"n\")) %>%\n  mutate(sam_ss = if_else(grepl(\"Social Sciences\", sam_division, fixed=TRUE), \"y\", \"n\")) %>%\n  mutate(sam_hu = if_else(grepl(\"Humanities\", sam_division, fixed=TRUE), \"y\", \"n\")) \n\n\n\n\n\n\nBy making these “y/n” fields it is possible to apply simple filters and count how many of the interactions fit a specific criterion. For example, the counting for one of the divisions is outlined below. The following code counts the number of NSE students that interacted with SAMs that have majors in the NSE division, the SS division, and the HU division. This may seem like a very tedious way to prepare the data, but it makes the creation of the alluvial plot extremely simple.\n\n\nstudent_nse_sam_nse <- sam_interactions %>%\n  filter(student_nse == \"y\") %>%\n  filter(sam_nse == \"y\") %>%\n  count()\n\nstudent_nse_sam_ss <- sam_interactions %>%\n  filter(student_nse == \"y\") %>%\n  filter(sam_ss == \"y\") %>%\n  count()\n\nstudent_nse_sam_hu <- sam_interactions %>%\n  filter(student_nse == \"y\") %>%\n  filter(sam_hu == \"y\") %>%\n  count()\n\n\n\n\n\n\nAfter calculating the frequency of each combination (of which there are 12!) we get the following table. This table was made using the DT library, which is very useful in making a quick table that looks really well and has great functionality!\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"nse\",\"nse\",\"nse\",\"ss\",\"ss\",\"ss\",\"hu\",\"hu\",\"hu\",\"un\",\"un\",\"un\"],[\"nse\",\"ss\",\"hu\",\"nse\",\"ss\",\"hu\",\"nse\",\"ss\",\"hu\",\"nse\",\"ss\",\"hu\"],[49,19,7,35,15,5,21,10,5,19,5,1]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>student<\\/th>\\n      <th>sam<\\/th>\\n      <th>count<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nAlluvial Plot\nNow comes the most exciting part: creating the plot! The ggalluvial package builds on ggplot and is relatively straightforward for those that have worked with ggplot2. We define the axis1 and axis2 fields as student and sam, since we are looking at the trends of students to SAMs. Then, geom_alluvium takes this along with geom_stratum to construct and format the flow and axes respectively. There are lots of ways to format the colors and labels of this plot by changing the arguments of the fill brewer, stratum, of alluvium!\n\n\nggplot(data,\n       aes(y = count, axis1 = student, axis2 = sam)) +\n  geom_alluvium(aes(fill = student, width = 1/12))+\n  geom_stratum(width = 1/12, fill = \"black\", color = \"grey\") +\n  scale_fill_brewer(type = \"qual\", palette = \"Set1\") +\n  geom_label(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  scale_x_discrete(limits = c(\"Student\", \"SAM\"), expand = c(.05, .05)) +\n  ggtitle(\"Students and SAMs\") +\n  theme_tufte()\n\n\n\n\nDoes division have an impact?\nThe above plot visually shows that there are larger chunks from each division of students that gravitate towards to the SAMs in the NSE division. However, this should be taken with some caution since the college’s overall student body has more participation in the NSE division than any other division. Because of this, the interactions may be expected to shift in this direction. Additionally, there are 15 SAMs that have academic interests in NSE, and 10 in SS, and 2 in humanities. These numbers may exceed the total number of SAMs because some SAMs are involved in multiple divisions, and thus double counted. Nonetheless, the numbers give an idea of where the SAM and students population stands in terms of the current popularity of the NSE majors.\nTo get a statistical number on the variation of students for a specific divison, we can run a chi-squared test.\n\\[\\chi^2 = \\sum_{set}\\frac{(Obs - Exp)^2}{Exp}\\]\n\n\n\nAs a start, we are assuming that students in the NSE division choose SAMs in the NSE, SS, or HU divisions at equal rates (the null hypothesis). Since we have 75 students in the NSE division (within the sample), our expected number of NSE students going to each SAM division would be 25. Therefore, our chi-squared value will be 37.44.\n\\[\\chi^2 = \\frac{(49 - 25)^2}{25} + \\frac{(19 - 25)^2}{25} + \\frac{(7 - 25)^2}{25} = 37.44\\] Now, to get the p-value, we calculate \\(P(\\chi^2_{n-1} \\ge 37.44) = 1 - P(\\chi^2_{n-1} \\le 37.44) = 1 - pchisq(37.44, df = 2) \\approx 7.4 \\times 10^{-9}\\). Since the p-value is quite significant, we can reject the null hypothesis that the NSE students seek advice from SAMs at an equal frequency across the divisions.\nThis same patter is repeated with SS students having a chi-square value of 18.67 and p-value of 8.842698910^{-5}. HU students have a chi-square value of 5.36 and p-value of 0.0685632. Undecided students have a chi-square value of 7.15 and a p-value of 0.0280622. It seems that humanities students are spread out closer to the null hypothesis assumption than the other divisions, meaning that humanities students seem to approach SAMs in different divisions with equally likely frequencies. Depending on the \\(\\alpha\\) cutoff that you prefer, the same may be said for undecided students. However, if we were going by the general \\(\\alpha < 0.05\\) rule, we can also reject the null hypothesis for undecided students and say that they do not seek advice in equal frequencies from SAMs in different divisions.\n\n\n\n",
    "preview": "posts/2021-04-25-sam-data-exploration/sam-data-exploration_files/figure-html5/making-plot-1.png",
    "last_modified": "2021-04-25T11:06:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-18-lesson-on-linear-regression/",
    "title": "Lesson on Linear Regression",
    "description": "An overview of Linear Regression using data from the Foundation of AIDS Research",
    "author": [
      {
        "name": "Elizabeth Shin",
        "url": {}
      }
    ],
    "date": "2021-03-16",
    "categories": [],
    "contents": "\n\nContents\nLinear Regression: An Overview\nImporting the Data\nEDA\nPlotting the Data\nFitting the Linear Model\nAssumptions\nBonus Analysis\n\nConclusion\n\nLinear Regression: An Overview\nLinear modeling is a fundamental aspect of statistics. One of the most ubiquitous predictive modeling methods, linear regression, is useful for its simplicity in implementation and interpretation. While linear regression is used to predict the value of a continuous response variable Y as a function of one or more explanatory variables X using a mathematical function, this technique operates with several underlying assumptions and is sensitive to outliers.\nLinear regression is useful for analyzing relationships among variables of interest and determining whether there is a linear or non-linear association among them, which is essential for exploratory data analysis and in choosing suitable models. To check whether the data we are attempting to model is suitable for linear regression, we must examine four assumptions which are summarized as L.I.N.E:\nLinear relationship [between the X and Y variables]\nIndependent observations [there is no relationship between the residuals and predictor variable(s)]\nNormal residuals [the distribution should be symmetric, unimodal, and approximately bell-shaped]\nEqual variance in residuals [there is no relationship between the residuals and fitted values]\nThe mathematical equation of linear regression can be modeled as:\n\\(Y = \\beta_1 + \\beta_2X + \\epsilon\\)\nwhere \\(\\beta_1\\) represents the intercept, \\(\\beta_2\\) signifies the slope, and \\(\\epsilon\\) stands for the error term. \\(Y\\) is the outcome variable, which is predicted by \\(X\\) as the input variable. Furthermore, there can be multiple \\(X\\) variables that are either continuous or categorical (i.e. \\(X_2\\), \\(X_3\\), etc.).\nAs an example, we will examine the Needle Exchange Data, a dataset gathered from amFAR (the Foundation of AIDS Research), to analyze how the distance to nearest syringe program (dist_SSP) relates to percent uninsured (pctunins). In the following overview, we will walk through how to perform EDA (Exploratory Data Analysis), review the L.I.N.E. assumptions,and fit the linear model to the data. In addition, we will provide guidance on data visualization and plot aesthetics. For the purpose of this example, we will only analyze the two variables mentioned above. Although it is relatively simple to add more explanatory variables to a linear model, analyzing and graphing the data can quickly become complicated. For further information, see this tutorial on Multivariate Regression: https://openintro.shinyapps.io/ims-04-multivariable-and-logistic-models-03/#section-multiple-regression\nImporting the Data\nTo preface this overview, we must install and load the following packages into R:\ntidyverse\ntidymodels\npatchwork\nmoderndive\nGGally\nreadxl\nFirst, load the data:\nNote: You might have to specify the file path to the data if it is not uploaded to a folder that you can directly access from your ‘Home’ page as is shown below.\nTo download these files to follow along with the exercise, go to this GitHub page for the .csv file and this page for the .xlsx file.\n\n\n\nWe named the Needle Exchange Data as needex and will refer to it henceforth when calling the data. The needex_codebook contains a description for the variable names used in needex (as well as many others). You can locate the variable names quickly in the codebook by searching using Ctrl + F.\nEDA\nTo start off, we should observe the variable types in the dataset:\n\n# A tibble: 6 x 7\n  county         STATEABBREVIATI… dist_SSP HIVprevalence opioid_RxRate\n  <chr>          <chr>               <dbl>         <dbl>         <dbl>\n1 wabasha county MN                   47.4          49.7           8.3\n2 johnson county GA                  125           283.            3  \n3 iron county    MO                   77.2         103.          131. \n4 kosciusko cou… IN                   42.1          52.1          64.2\n5 delaware coun… OK                   49.2          77.9          78.7\n6 scurry county  TX                  141            70.7          81.5\n# … with 2 more variables: pctunins <dbl>, metro <chr>\n\nUsing the head() function, we can observe the first 6 rows and each variable name in the dataset. You may also specify the number of rows of data shown using n = ..., but the default is 6 rows.\nNotice that we have three categorical variables: county, STATEABBREVIATION, and metro. Our four numerical variables are as follows: dist_SSP, HIVprevalence, opioid_RxRate, and pctunins.\nYou could check the needex_codebook for the descriptor for each variable, but for the sake of brevity we will clarify several of the variables:\ndist_SSP = distance to nearest syringe services program (in miles)\nHIVprevalence = # living people diagnosed w/ HIV per 100,000 (including adults and adolescents 13+)\nopioid_RxRate = # opioid prescriptions per 100 people\npctunins = % civilian population (not in an institution) w/o health insurance coverage\nmetro = whether the geographical location is considered metropolitan or not (NOTE: This is a personal interpretation because the needex_codebook did not provide a definition)\nWe can then create a pairs plot - using the GGally package - to display a scatterplot matrix of all the quantitative variables from the needex dataset.\n\n\n\nLooking at the pairs plot, it appears that the strongest correlation of 0.0.413 among the quantitative variables is between dist_SSP and pctunins (shown at the top right) while the scatterplot of that relationship is displayed in the bottom left. Typically, a correlation coefficient between 0.3 to 0.7 is considered to be of moderate strength. Thus, we can say that the relationship between dist_SSP and pctunins is a positive, moderate correlation.\nWhile you knew from earlier that we were going to analyze the relationship between dist_SSP and pctunins, we have shown through EDA how you can explore the relationships among your variables to determine which variables are appropriate to focus on and model further.\nThe code for producing a scatterplot of dist_SSP and pctunins is shown:\n\n\n\nPlotting the Data\nDoes the scatterplot above seem a bit hard to interpret without context? It should. In R, you can visualize your graph in countless ways - adjusting the parameters, changing the titles and labels, layering geoms and aesthetics, adding colors and gradients and shapes and text, etc. We should start by changing the labels of the x- and y-axis to more appropriate titles.\n\n\n\nTo make the plot more aesthetically pleasing, you may also use the theme() function to adjust the size, color, font, and remove/add other features of the text in the labels.\n\n\n\nWe should add a least-squares line to the graph since we have already determined that there is a positive, moderate correlation between the explanatory and response variable. In addition, if you would rather not manually change the aesthetics of your graph, you can use ggthemes to choose from readily available formats at your preference. For an overview of available themes, you can check out https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/. To reduce the amount of code written in the following chunk, I selected an arbitrary theme. I added the least-squares line using stat_smooth(method = \"lm\"), which instructs R to fit a linear model to the data.\n\n\n\nOne thing you should notice is that the standard error remains relatively small around 3 - 20 percent. The standard error increases around the least-squares line at higher percentages of civilians without health insurance. This is a result of less values located between 20 - 36%. Standard error increases when standard deviation (variance of a population) increases, which occurs when the sample size decreases. You could possibly interpret that the larger standard error between 20 - 36 % percent comes from the smaller sample size within that range. Since the data clusters between 3 - 20 %, the standard error is smaller.\nIf you look closely, you’ll see that no dots appear between 0-3%. How did we know that the cutoff of the least-squares line was around 3 percent instead of 2 or 2.5 (which isn’t readily observable by the tick marks along the x-axis)? You can use the range() function to find the minimum and maximum of a vector. Out of curiosity, we can also examine the min and max of the outcome variable as well.\n\n[1]  3.0 35.9\n[1]   0 510\n\nInteresting! It appears that the range of people without health insurance falls between 3 - 36 %, while the distance to the nearest syringe program can range between 0 - 510 miles. 510 miles is longer than the length of Florida!\nHowever, this only tells us so much about the data. To gain more insight, we will fit the linear model to the data and check the L.I.N.E. assumptions to determine how well the model suits the data.\nFitting the Linear Model\nWe fit our model to the variables of interest:\n\n# A tibble: 2 x 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  <chr>        <dbl>     <dbl>     <dbl>   <dbl>    <dbl>    <dbl>\n1 intercept    12.5     10.2        1.23   0.221    -7.51    32.5 \n2 pctunins      7.82     0.773     10.1    0         6.30     9.34\n# A tibble: 1 x 1\n    cor\n  <dbl>\n1 0.413\n\nFitting the model doesn’t mean that R will automatically display the values of that linear model to us. We used the function get_regression_table() to output the regression table. We then calculated the correlation between dist_SSP and pctunins and got 0.4126744 in R Markdown. Look familiar? It’s exactly the same as the 0.413 correlation from the pairs plot! We just wanted to demonstrate how to calculate the correlation between and among all the numerical variables of needex manually, yet also how much simpler (and faster!) GGally package can make it for us! The beauty of R is how versatile it is in terms of function and accessibility to statisticians of all levels!\nAssumptions\nNow that we have performed EDA, plotted the linear relationship between the explanatory X variable and outcome Y variable, and fitted the model to the data, we must check how suitable the model by checking the L.I.N.E. assumptions.\nThere must be a Linear relationship between independent and dependent variables.\nWe have already shown this multiple times above, but we will include a graph here. We will format this in a new theme for fun since we’re in a new section. In addition, if you wish to remove the standard error of the line from the scatterplot, then simply set se = FALSE inside stat_smooth().\n\n\n\nWhile we know that the positive correlation is only of moderate strength, it was the strongest correlation among the numerical variables of needex and so was the most suited for a linear model. Just to double check, let’s layer other models to see if they fit the data well.\n\n\n\nThe General Additive Model (GAM) is an extension of linear models with a smoothing function. Thus, it doesn’t differ much from the Linear Regression model using the ordinary least squares (OLS) line. However, the Local Regression (LOESS) model jumps up and down a lot from about 3 - 15 %. LOESS is used to fit multiple regressions in a local range, which is most often applied to smooth a fluctuating time series. In this case, it doesn’t add much relevant detail to the scatterplot at first glance, and this is a scatterplot instead of a time series.\nLet’s check the other assumptions.\nThe observations must be Independent such that there is no relationship between residuals and the predictor X variable.\n\n\n\nWhile at first glance it appears that this scatterplot displays a slight downwards correlation (due to the cluster of points below 0 on the y-axis and between 3 - 20% on the x-axis), we should check the correlation between the predictor variable and the residuals as well as add an OLS line.\n\n# A tibble: 1 x 1\n        cor\n      <dbl>\n1 -5.14e-16\n\n\nSurprise (or maybe not)! The correlation coefficient is quite low (orders of magnitude away from a concerning value for the assumption) and adding the line shows a near perfect horizontal slope, which indicates that there is no relationship between the % uninsured and residuals. The observations are independent of each other such that the order in which the data was collected does not matter.\nA Normal distribution of residuals would exhibit a symmetric, unimodal, bell-shaped curve.\n\n\n\nWhile the distribution seems slightly right-skewed, the distribution is unimodal and roughly bell-shaped. We can note these observations, but overall shouldn’t be highly concerned about the shape of this particular distribution.\nWe should see Equal variance in residuals. In a scatter plot of residuals by fitted values, there should be no relationship. The fitted values are simply the predicted response values given an input.\n\n\n\nThere appears to be no relationship between the residuals and fitted values, which indicates that the residuals vary equally.\nIf you would also like to see the plots of the assumptions side by side, we can combine these ggplots using one of the features of the patchwork package. One thing to note is that the plots for ‘Independent Observations’ and ‘Variance in Residuals’ look very similar to one another, which is not always the case. Certainly we would want to see no relationships between the axes, but this does not guarantee that the plots would look similar.\n\n\n\nIn fact, here’s an image from a lab that displays non-similar plots for the assumptions of Independent Observations (leftmost graph) and Equal Variance in Residuals (rightmost graph) from http://www.swarthmore.edu/NatSci/aluby1/stat041/Labs/Lab05-ols.html#.\n\n\n\nNow let’s take a look at the coefficients of our linear model:\n\n# A tibble: 2 x 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)    12.5     10.2        1.23 2.21e- 1    -7.51     32.5 \n2 pctunins        7.82     0.773     10.1  5.56e-22     6.30      9.34\n\n\n\n\nCoefficient plots are helpful in visualizing estimates in a regression model (usually with many parameters) in terms of uncertainty and magnitude of effect - which is not really shown here since we only included one estimate aside from the intercept. They are also used to compare the estimates of models, in which smaller confidence intervals indicate less uncertainty. For more information and in-depth examples exploring the mentioned uses of coefficient plots, you can visit https://cran.r-project.org/web/packages/jtools/vignettes/summ.html.\n\nCongratulations! You have performed all four L.I.N.E. checks, which indicates that using a linear regression model was an appropriate choice to fit to this data.\n\nBonus Analysis\nSo far, we’ve only explored the linear relationship between dist_SSP and pctunins - two numerical variables of needex. Let’s check out the relationship between pctunins and metro. Since metro is a categorical variable and binary (metro or non-metro), we should plot the data as a boxplot instead.\n\n\n\nNote that the vertical lines in the middle of the boxplots represents the median, not the mean. Instead, the diagonal red line connects the mean of the non-metro and metro groups.\nThe median (and mean) percentage of uninsured people of non-metro appear(s) somewhat higher than metro. For statistical analyses, instead of using a linear model, this calls for a t-test - or more specifically, a two-sample t-test. This t-test will compare the two means to test whether the mean of the non-metro group is significantly different from that of the metro group.\n\n\n    Two Sample t-test\n\ndata:  pctunins by metro\nt = -4.1892, df = 498, p-value = 3.312e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.7055006 -0.9779436\nsample estimates:\n    mean in group metro mean in group non-metro \n               11.17434                13.01606 \n\nThe means of the two groups are extremely statistically significant (P = 0.0003) from each other. In addition, we can specifically analyze a subset of the data using the filter() function. For instance:\n\n# A tibble: 6 x 7\n  county         STATEABBREVIATI… dist_SSP HIVprevalence opioid_RxRate\n  <chr>          <chr>               <dbl>         <dbl>         <dbl>\n1 lebanon county PA                   60.2          39.7          48.1\n2 york county    PA                   42.1          70.8          51.6\n3 huntingdon co… PA                   95.5          48.1          49.9\n4 fayette county PA                   23.2          27.1         107  \n5 clearfield co… PA                   88            25.8          61.3\n6 susquehanna c… PA                   28.9          27.9          48  \n# … with 2 more variables: pctunins <dbl>, metro <chr>\n\n    Two Sample t-test\n\ndata:  pctunins by metro\nt = -0.92033, df = 12, p-value = 0.3755\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.656068  1.484639\nsample estimates:\n    mean in group metro mean in group non-metro \n               8.571429                9.657143 \n\nIncredible! When comparing the mean percentages of uninsured people from non-metro and metro areas in PA, they are not statistically significant from each other (P = 0.3755)even though the non-metro mean is still a bit higher. This also demonstrates how the overall patterns found in the whole dataset might mask individual trends in subsets and highlights the importance of analyzing data on multiple levels.\nConclusion\nOur hope is that with this exercise, you have developed a greater understanding of linear regression and how it is constructed in R. We have reviewed how to import packages and data into R, perform steps to EDA, adjust aesthetic features of plots, work through stages of fitting a model and check assumptions, and even briefly include how to build and interpret a two-sample t-test. While the analyses of this data led to interesting results, what is more important (and exciting!) is that you are now able to apply what you have learned to construct a linear regression model and test its suitability for almost any dataset. In the scenario that using a linear model is not appropriate, you can look to logistic regression, transformation, or segmentation/clustering of data.\nIf you’re interested in learning more information about linear regression, see here: http://r-statistics.co/Linear-Regression.html\n\n\n\n",
    "preview": "posts/2021-03-18-lesson-on-linear-regression/lesson-on-linear-regression_files/figure-html5/All-Models-1.png",
    "last_modified": "2021-03-18T17:43:26-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-05-bls-earnings-data/",
    "title": "BLS Earnings Data",
    "description": "Visualizing the gender wage gap for #TidyTuesday -- includes a demo of customizing ggplots!",
    "author": [
      {
        "name": "Amanda Luby",
        "url": {
          "www.swarthmore.edu/NatSci/aluby1": {}
        }
      }
    ],
    "date": "2021-03-05",
    "categories": [],
    "contents": "\nThe Graph\n\n\n\nCode Walk through + Customization\nNote: For some of these themes, you’ll need to install the ggthemes package. I also use the fill argument in my graph instead of color, and it can be hard to remember if you need to use scale_fill_X or scale_color_X! I also couldn’t get the custom font (Avenir) to show up in the flipbook, so just used the “serif” option.\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\n\n\n\n",
    "preview": "posts/2021-03-05-bls-earnings-data/bls-earnings-data_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-03-05T17:30:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-05-dubois-challenge/",
    "title": "DuBois Challenge",
    "description": "A recreation of a Du Bois line chart for #TidyTuesday",
    "author": [
      {
        "name": "Atesh Camurdan",
        "url": {}
      }
    ],
    "date": "2021-03-05",
    "categories": [],
    "contents": "\nThis graph is a representation of growth in population of the Colored and White races over time in the state of Georgia. Over the course of making this graph, I learned a lot of tricks about how to use R to plot, theme, and write legends. It was a struggle, essentially starting from scratch, but I am pleased to have come up with this result and look forward to creating more graphs in the future!\n\n\n\nOriginal Graph:\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-05-dubois-challenge/dubois-challenge_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-03-05T13:40:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-18-dubois-challenge/",
    "title": "DuBois Challenge",
    "description": "A recreation of DuBois' \"Proportion of Freemen and Slaves Among American Negroes\" for #TidyTuesday week of Feb 16, 2021",
    "author": [
      {
        "name": "Elizabeth Shin",
        "url": {}
      }
    ],
    "date": "2021-03-05",
    "categories": [],
    "contents": "\nAbout:\nThis graph is a recreation of one of the famous W.E.B. DuBois graphs in response to the #TidyTuesday challenge. In particular, I chose challenge 4, which calls to replicate the graphic called “Proportion of Freemen and Slaves Among American Negroes”. The following plot serves as an illustration of the proportion of freemen to slaves among African Americans from 1790-1870.\nNOTE: The word “Negroes” appears in the title of the original graph and in my recreation. This is not a term I take lightly and is a phrase that Du Bois references in his data visualizations. Since the purpose of this graph is to replicate the work of Du Bois as part of the #TidyTuesday challenge, the usage of the word here is only meant to acknowledge and contextualize his use of language for this post.\nCheck out my GitHub to view the source code!\nPlot 1: Original Graph by DuBois\n\nPlot 2: Example of a Recreation by Anthony Starks\nThis recreation is (unfortunately!) not mine. I was inspired to choose this graph for the challenge after seeing the following replication in this post using a data visualization tool called decksh. I wanted to see if I could reproduce a similar plot.\n\nPlot 3: Personal Replication of the Graph\nHere is my personal reproduction:\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-18-dubois-challenge/plots/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-18T17:30:11-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-03-happiness-trends-in-music/",
    "title": "Happiness Trends in Music",
    "description": "An exploration of Spotify data alongside global happiness and crime data.",
    "author": [
      {
        "name": "Tolga Atabas",
        "url": {}
      },
      {
        "name": "Sherry Huang",
        "url": {}
      }
    ],
    "date": "2021-03-03",
    "categories": [],
    "contents": "\nExternal Link to Project\nGithub\nFull Report\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-03-happiness-trends-in-music/world-happiness-map.png",
    "last_modified": "2021-03-03T16:00:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-26-mlb-home-runs/",
    "title": "MLB Home Runs",
    "description": "Replicating and redesigning a visualization on home runs in the MLB.",
    "author": [
      {
        "name": "Brooke Coneeny",
        "url": {}
      }
    ],
    "date": "2021-02-26",
    "categories": [],
    "contents": "\nAbout:\nFor this project I chose to replicate and re-design a visualization which shows how the number of MLB home runs per year has changed over time.\nLink to the article containing the original graph and data:\nReplica Graph:\n\n\n\n\n\n\nRe-Designed Graph:\n      The original data set consisted of 2 variables (Year & Number of HRs) and 149 observations (1871-2020). I created a new variable, Era, which categorized what era of baseball each of the years fell into.\n      After I read the article the graph originated from, “A look into Major League Baseball’s massive home run splurge in 2019,” written by Ben Cooper, I learned that the main takeaway from the graph was intended to be the recent rise in home runs in the tail end of the Post-Steroid era. Avid baseball fans would know that after the MLB began testing for steroids, there was a long decrease in home runs, and bats did not fully come back alive until around 2016, and mostly 2019.\n      The original graph did display the increase; however, I created a new variable in the data set that accounted for Era to better explain the trend. I used the geom_col function because it allowed me to plot the year vs the number of home runs and fill the columns in by era. The reason I wanted to fill the columns by era was because it allows the reader to understand why there was a big drop in HRs followed by the highest peak in HRs ever recorded (the reason being because of the transition from the steroid to post-steroid eras). I also chose to use geom_line in addition to geom_col for the glow-up visualization. Since the columns are used to show the change in eras, the line is being used to show the trend between years. It is critical that visualizations must always be insightful, meaning they must help viewers make discoveries otherwise inaccessible. I believe that by creating an additional variable for the data set I made a more insightful graph for readers.\n      I also made some aesthetic changes to the graph, the first being the change in text. I chose a different custom font which I thought was more appealing and also increased the size of the title to make it easier for readers’ eyes. I also created a subtitle, to explain the variables in more detail. A change in background was also needed, since the stark white used in the original graph was too bright, so I replaced it with a subtle grey. Getting rid of the the axes was another change I made, for they were unnecessary for the point that was being proven.\n\n\n\nAppendix: Code\n\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\n#Load in data\nlibrary(readxl)\nhomeruns <- read_excel(\"HOMERUNS.xlsx\")\n#Create Replica Plot\nlibrary(tidyverse)\nlibrary(directlabels)\nggplot(data = homeruns, mapping = aes(x = Year, y = HR)) +\n  geom_line(color = \"skyblue\", size = 1) +\n  scale_x_continuous(breaks = seq(1880, 2020, 10)) +\n  scale_y_continuous(breaks = seq(0,6500,500)) +\n  labs(title = \"MLB Home Runs by Year\", size = 5) +\n  theme_classic() +\n  theme(\n    panel.grid.major.x = element_blank(), \n    panel.grid.major.y = element_line(size = .1, color = \"lightgrey\"),\n    axis.title.y = element_blank(), \n    axis.title.x = element_blank(),\n    axis.ticks = element_blank(),\n    axis.line.y = element_blank(),\n  ) +\n  annotate(\"text\", x = 2020, y = 7000, label = \"HR\")\n#create Glow-Up plot\nlibrary(extrafont)\nggplot(data = homeruns) +\n  geom_col(mapping = aes(x = Year, y = HR, fill = Era)) +\n  scale_fill_brewer(palette = \"Paired\") +\n  geom_line(mapping = aes(x = Year, y = HR), color = \"black\") +\n  scale_x_continuous(breaks = seq(1880, 2020, 10)) +\n  scale_y_continuous(breaks = seq(0,6500,500)) +\n  labs(\n    title = \"MLB Home Runs by Year\", \n    subtitle = \"Total number of combined home runs by all teams in the MLB, \\ncategorized by Baseball-Era\"\n    ) +\n  theme(\n    plot.background = element_rect(fill = \"gainsboro\"),\n    panel.background = element_rect(fill = \"gainsboro\"),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    text = element_text(family = \"Baskerville\", size = 10),\n    plot.title = element_text(size = 20),\n    legend.background = element_rect(fill = \"gainsboro\")\n  )\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-26-mlb-home-runs/mlb-home-runs_files/figure-html5/glow-up-1.png",
    "last_modified": "2021-02-26T15:33:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-26-dubois-challenge/",
    "title": "Du Bois Challenge",
    "description": "A recreation of a Du Bois bar chart for #TidyTuesday week of Feb 16, 2021",
    "author": [
      {
        "name": "Amanda Luby",
        "url": {
          "https://swarthmore.edu/NatSci/aluby1": {}
        }
      }
    ],
    "date": "2021-02-16",
    "categories": [],
    "contents": "\nOriginal Image\nThe original chart was prepared by Du Bois for the Negro Exhibit of the American Section at the Paris Exposition Universelle in 1900 to show the economic and social progress of African Americans since emancipation. This particular graph shows the marital status of African Americans compared to Germans, by age group.\n\n\n\nReplication\nWe can replicate the graph in ggplot2 using a facetted barplot with custom colors and fonts. While this isn’t a perfect recreation, it’s true to the style of the original. Many thanks to the folks at TidyTuesday as well as Anthony Starks, Allen Hillery, and Sekou Tyler for putting the challenge together (including creating a Du Bois style guide)!\n\n\n\nCode\n\n\nknitr::opts_chunk$set(echo = FALSE, fig.path = \"plots/\", fig.width = 8, fig.height =5)\nlibrary(tidyverse)\n\n#tuesdata <- tidytuesdayR::tt_load('2021-02-16')\nconjugal <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-16/conjugal.csv')\nknitr::include_graphics(\"original-graph.png\")\ndubois_color = c(\"#4a6554\", \"#f1b834\", \"#b53541\")\nbg_color = \"#e4d7ca\"\n\nconjugal %>%\n  pivot_longer(-c(Population, Age), names_to = \"status\", values_to = \"percent\") %>%\n  mutate(\n    Population = fct_rev(toupper(Population)),\n    status = toupper(status),\n    Age = fct_inorder(case_when(\n      Age == \"15-40\" ~ \"AGE\\n15-40\",\n      Age == \"40-60\" ~ Age,\n      TRUE ~ \"60\\nAND\\nOVER\"\n    ))\n  ) %>%\n  ggplot(., aes(y = Population, x = percent, fill = status)) + \n  geom_col(width = .3) + \n  geom_text(aes(label = paste0(percent, \"%\")), position = position_stack(vjust = .5), size = 4, family = \"Bai Jamjuree\", color = \"gray25\") +\n  facet_grid(rows = vars(Age), switch = \"y\") + \n  labs(\n    title = \"CONJUGAL CONDITION .\",\n    y = \" \",\n    x = \" \",\n    fill = \"\"\n  ) +\n  scale_fill_manual(values = dubois_color,\n                    guide = guide_legend(override.aes = list(shape = 21),\n                                         reverse = TRUE)) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    panel.background = element_rect(fill = bg_color, color = bg_color),\n    plot.background = element_rect(fill = bg_color),\n    panel.grid = element_blank(),\n    panel.border = element_blank(),\n    strip.text.y.left = element_text(angle = 0, color = \"gray25\"),\n    axis.text.y = element_text(color = \"gray25\"), \n    axis.text.x = element_blank(),\n    strip.placement = \"outside\",\n    text = element_text(family = \"Bai Jamjuree\", color = \"gray25\", size = 14),\n    plot.title = element_text(size = 16, face = \"bold\", hjust = .5, family = \"Airborne II Pilot\"),\n    panel.spacing = unit(2, \"lines\")\n  )\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-26-dubois-challenge/original-graph.png",
    "last_modified": "2021-02-26T13:08:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-26-two-sample-t-tests-presidential-pardon-data/",
    "title": "Comparing Sample Means: Presidential Pardon Data",
    "description": "An Example using Two-Sample T-Tests.",
    "author": [
      {
        "name": "Quinn Basewitz",
        "url": {}
      },
      {
        "name": "Sydney Levy",
        "url": {}
      }
    ],
    "date": "2021-01-29",
    "categories": [],
    "contents": "\nToday we will be looking at how to compare quantitative measurements for two populations! This procedure is called a two-sample t-test and will determine whether there is a notable difference between the mean of two populations. While walking through how to do this, we will be using presidential pardon data in a dataset containing the name of each president, the year the presidency began, the number of years in office, the political party, and the number of pardons granted during their presidency. We will also construct a variable that calculates the number of pardons per year in office, to account for varying term lengths.\nHere is a quick look at the data and the code for a few of the transformations we made:\n\n\n#Load in the dataset\npresidential_pardons = read_csv(\"presidentialpardons.csv\")\n\n#Remove independents:\npresidential_pardons_dem_rep <-subset(presidential_pardons, party!= \"Independent\")\n\n#Convert party to categorical variable\npresidential_pardons$party <- as.factor(presidential_pardons$party)\n\n#Create new variable with pardons per year\npresidential_pardons_dem_rep = presidential_pardons_dem_rep %>% \n  mutate(pardons_per_year = pardons/years_in_presidency)\n\n#Summary of first few observations\nhead(presidential_pardons_dem_rep)\n\n\n# A tibble: 6 x 7\n  name  year_presidency… pardons years_in_presid… party og_party\n  <chr>            <dbl>   <dbl>            <dbl> <chr> <chr>   \n1 Andr…             1829     386                8 Demo… Democrat\n2 Mart…             1837     168                4 Demo… Democrat\n3 Jame…             1845     268                4 Demo… Democrat\n4 Fran…             1853     142                4 Demo… Democrat\n5 Jame…             1857     150                4 Demo… Democrat\n6 Abra…             1861     343                4 Repu… Republi…\n# … with 1 more variable: pardons_per_year <dbl>\n\nIn order to understand how two-sample t-tests fit into the larger framework of hypothesis testing you have learned, we can use the following framework. First, consider whether the variable of interest is categorical (a proportion) or quantitative (a mean). If the variable is categorical, you will be running a z-test and if it is quantitative you will be running a t-test. Next, consider the number of populations of interest. If you are using a single population the test of interest will be a one-proportion z-test or a one-sample t-test. Similarly, with two populations you run a two-proportion z-test or two-sample t-test.\nIn the dataset we will focus on, we have two populations: Republican presidents and Democratic presidents. The mean that we will be looking at for each is the mean number of presidential pardons granted during their presidencies, scaled by the number of years in office. Thus, our data fit the framework for a two-sample t-test.\nBecause the data is not paired, and we are not comparing a single data point in both populations, we will skip paired t-tests for now and will dive into the two-sample t-test. In our case, the sample we are working with is the whole population since we have data for every US President. But, for this example, let’s assume there are actually hundreds of Democratic and Republican presidents and our dataset is just a random sample of those hundreds. By looking at the sample and comparing the means with our two-sample t-test, we can determine whether there is a significant difference in the means for these entire populations, using the data in our sample.\nOur null hypothesis, as we’ve seen before, assumes that there is no difference between the two groups. So, since we are comparing the means of two populations, \\(H_o: \\mu_1 - \\mu_2 = 0\\). Our alternative hypothesis is that there is a notable difference between the means, and is \\(H_a: \\mu_1 - \\mu_2 \\neq 0\\).\nFor the purpose of clarity, we will refer to the mean number of pardons by Republican presidents as \\(\\mu_R\\) and the mean number of pardons by Democratic candidates as \\(\\mu_D\\). Thus our null hypothesis becomes \\(H_o: \\mu_R - \\mu_D = 0\\). Similarly, the alternative hypothesis becomes \\(H_a: \\mu_R - \\mu_D \\neq 0\\).\nLet’s explore our data a little more in the following graphics. The first is interactive, and you can move your mouse over each point to see who the president is. The second is an animation, so that you can see changes over time. The third graph is a boxplot displaying the same data, so that the distribution for each party is visible.\nPlotly Graph\n\n\n{\"x\":{\"data\":[{\"x\":[1829,1837,1845,1853,1857,1885,1913,1933,1945,1961,1963,1977,1993,2009],\"y\":[48.25,42,67,35.5,37.5,138.375,310,307.25,255.5,287.5,197.833333333333,141.5,57.375,240.875],\"text\":[\"Andrew Jackson\",\"Martin Van Buren\",\"James K. Polk\",\"Franklin Pierce\",\"James Buchanon\",\"Grover Cleveland\",\"Woodrow Wilson\",\"Franklin D. Roosevelt\",\"Harry S. Truman\",\"John F. Kennedy\",\"Lyndon B. Johnson\",\"Jimmy Carter\",\"Bill Clinton\",\"Barack Obama\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,255,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,255,1)\"}},\"hoveron\":\"points\",\"name\":\"Democrat\",\"legendgroup\":\"Democrat\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1861,1869,1877,1881,1881,1889,1897,1901,1909,1921,1923,1929,1953,1969,1974,1981,1989,2001,2017],\"y\":[85.75,166.5,223.25,0,96.2857142857143,153.25,229.5,122.625,189.5,533.333333333333,237.692307692308,346.25,144.625,168.363636363636,163.6,50.75,19.25,25,59.25],\"text\":[\"Abraham Lincoln\",\"Ulysses S. Grant\",\"Rutherford B. Hayes\",\"James A. Garfield\",\"Chester A. Arthur\",\"Benjamin Harrison\",\"William McKinley\",\"Theodore Roosevelt\",\"William Taft\",\"Warren Harding\",\"Calvin Coolidge\",\"Herbert Hoover\",\"Dwight D. Eisenhower\",\"Richard Nixon\",\"Gerald Ford\",\"Ronald Reagan\",\"George  H. W. Bush\",\"George W. Bush\",\"Donald Trump\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(255,0,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(255,0,0,1)\"}},\"hoveron\":\"points\",\"name\":\"Republican\",\"legendgroup\":\"Republican\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":14.6118721461187},\"title\":{\"text\":\"Chronological Distribution of Presidential Pardons\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":17.5342465753425},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[1819.6,2026.4],\"tickmode\":\"array\",\"ticktext\":[\"1850\",\"1900\",\"1950\",\"2000\"],\"tickvals\":[1850,1900,1950,2000],\"categoryorder\":\"array\",\"categoryarray\":[\"1850\",\"1900\",\"1950\",\"2000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"serif\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Year Presidency Began\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-26.6666666666667,560],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"200\",\"400\"],\"tickvals\":[0,200,400],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"200\",\"400\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"serif\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"Pardons Per Year\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":11.689497716895},\"y\":0.891732283464567},\"annotations\":[{\"text\":\"Political Party\",\"x\":1.02,\"y\":1,\"showarrow\":false,\"ax\":0,\"ay\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":14.6118721461187},\"xref\":\"paper\",\"yref\":\"paper\",\"textangle\":-0,\"xanchor\":\"left\",\"yanchor\":\"bottom\",\"legendTitle\":true}],\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"807694df66d\":{\"x\":{},\"y\":{},\"colour\":{},\"text\":{},\"type\":\"scatter\"}},\"cur_data\":\"807694df66d\",\"visdat\":{\"807694df66d\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\n\n\n\n\nIn order to run our two-sample t-test, we need the sample mean, \\(\\bar{x}\\), the number of observations, n, and the standard deviation, s, for each sample. In order to find the number of observations for each party, we can use the tally() function from the dplyr library.\n\n\n#Find sample sizes for each party\nlibrary(dplyr)\npresidential_pardons %>%\n        group_by(party) %>%\n        tally()\n\n\n# A tibble: 3 x 2\n  party           n\n  <fct>       <int>\n1 Democrat       14\n2 Independent    11\n3 Republican     19\n\nFrom this, we can see that there are 14 past Democratic presidents, 19 Republican presidents, and 11 presidents from any other party. For the purpose of our analysis, we will only focus on the Democratic and Republican presidents. Thus \\(n_D = 14\\) and \\(n_R = 19\\). To find the sample means and standard deviations we can use R’s summarise function:\n\n\n#Find sample means and standard deviation by party\npresidential_pardons_dem_rep %>%\n   group_by(party) %>%\n   summarise(mean = mean(pardons_per_year), sd = sd(pardons_per_year))\n\n\n# A tibble: 2 x 3\n  party       mean    sd\n  <chr>      <dbl> <dbl>\n1 Democrat    155.  109.\n2 Republican  159.  126.\n\nFrom running the above command, we now have that \\(\\bar{x}_R = 158.67\\) and \\(\\bar{x}_D = 154.75\\) and \\(s_R = 126.24\\) and \\(s_D = 108.86\\).\nIn order to do a two-sample t-test, a few assumptions must be met. Just as in a one-sample t-test, the same three conditions must be met: independence assumptions within groups, the nearly normal condition, and the randomization condition. In addition, the data must fulfill the independent group assumption, as the two groups (Democrats and Republicans), must be independent of each other.\n\n\n\nIt’s always important to check if your data looks nearly normal in a histogram. In our case, the distributions don’t necessarily look normal, but for the purpose of this project we will assume that the nearly normal condition is met. Ideally, we would have a larger sample size in order to meet the Central Limit Theorem for normal bell-shaped data. But, since we were limited by the number of presidents the country has had, we will assume that the condition is met and proceed with our test.\nFor the purposes of our t-test, we will assume the number of pardons one Democratic or Republican president granted did not affect the number of pardons other presidents of their party granted, in order to meet the independence within groups condition.\nFor the randomization condition, we would hope that each observation is taken by random sample from the population. Again, in this case, we have data from every member of the population since we have every president, but we have previously assumed for the purpose of this project that this is a random sample.\nNow, we can run our two-sample t-test!\n\n\n#2 sample t-test for pardons per year\nresult_pardons_per_year <- t.test(pardons_per_year ~ party, data = presidential_pardons_dem_rep, \n                 var.equal = TRUE)\n\nresult_pardons_per_year\n\n\n\n    Two Sample t-test\n\ndata:  pardons_per_year by party\nt = -0.093446, df = 31, p-value = 0.9262\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -89.59820  81.74751\nsample estimates:\n  mean in group Democrat mean in group Republican \n                154.7470                 158.6724 \n\nViewing the output of the two-sample t-test, like with any of the tests we have looked at previously, the p-value tells us whether to accept or reject the null hypothesis. We will use an alpha-level of 0.05 as the deciding limit. Our p-value of .9262 means that there is a 92.62% chance that we would get results similar to what we have, with the true value of the means not being significantly different. Since our p-value is definitely not under 0.05, we do not reject the null hypothesis. Thus, there is insufficient evidence to prove that the mean pardons per year for the two parties is substantially different.\nHopefully, this walk through of performing a two-sample t-test with real data was helpful for you, and you are able to see why hypothesis testing can be so useful in the real world!\n\n\n\n",
    "preview": "posts/2021-02-26-two-sample-t-tests-presidential-pardon-data/two-sample-t-tests-presidential-pardon-data_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-02-26T12:59:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-26-california-forest-fires/",
    "title": "California Forest Fires",
    "description": "A visual exploration of wildfires between 2013-2019.",
    "author": [
      {
        "name": "Brooke Coneeny",
        "url": {}
      },
      {
        "name": "Sydney Levy",
        "url": {}
      }
    ],
    "date": "2021-01-28",
    "categories": [],
    "contents": "\nIntroduction\n      California is home to some of the deadliest and most destructive wildfires in the world. Although every year the state experiences this misfortune, the 2020 California wildfire season set all time records. In fact, five out of the six largest fires in the state’s history were recorded during 2020 (Guardian). The damage caused by the wildfires extended far beyond the burning of millions of acres of trees. In addition to the environmental impact of the wildfires, living conditions became unbearable for many California residents. For some, the air quality was so poor that people could not even leave their homes without having to worry about smoke inhalation. Researchers have even been led to believe that the smoke and air quality led to hundreds of excess deaths in California cities (Guardian). Because of this recent and ongoing tragedy, we wanted to learn more about the history of wildfires in the great state of California.\n      To dive deeper into California’s history of wildfires, we found a dataset that contains detailed records of wildfires that took place in California between 2013 and 2019. The dataset comes from the website Kaggle.com which serves as a platform to publish datasets and allows users to build and publish data models. While the original dataset contained observations for over 1,600 wildfires, we chose to simplify the dataset. We did so by removing all rows that did not have an observation for the number of crews involved in fighting the fire, resulting in 171 observations. Further, the original dataset included 40 different variables of study however we decided to focus on the 18 variables that would best allow us to analyze the social and environmental impact of the fires.\n      The dataset includes the latitude, longitude, and county where each wildfire took place. We will use these variables to understand the role location plays in the amount of destruction caused by the wildfires. We also plan to study the impact that the fires play on individual’s lives through the number of injuries and fatalities that were caused by the fires as well as the number of structures that were damaged or destroyed. Further, the number of administrators on duty, personnel involved, fire crews involved, fire engines, helicopters and water tenders in the dataset will allow us to learn more about the resources that are required and supplied in order to fight the fires. We will also look to better understand the year to year differences between wildfires reported in California as well as the county to county differences. Finally, through analyzing the start and end dates of the wildfires as well as the number of acres burned, we will focus on the amount of damage done based on the duration of the fires.\n      There is clearly lots of data to work with, however we will primarily focus on the location, damage caused, and resources required by the wildfires. We plan to create proportional symbol maps to analyze the geographic impact of the California wildfires throughout the state. By doing this, it will help draw attention to the areas that need support the most. A visualization of this type will also aid those who live in the state and show them what areas are most prone to wildfires. Additionally, scatterplots will allow us to highlight the correlation between the amount of resources provided to a wildfire and the fire’s duration. We will also compare the scatterplots by county, in order to investigate whether the counties with the highest amount of damage are receiving the most care. Further, boxplots will highlight the differences between the median durations of fires across the years in study, giving us a look into the trends across the state as a whole. Through this analysis we hope to deepen public understanding of the wildfires as the trends in damage caused by the wildfires has been progressing in recent years.\n      While the 2020 wildfire season was the worst ever recorded, the numbers in the years prior have not been much better. The size and frequency of the wildfires have been gradually worsening in the years prior to 2020, ranking as some of the largest and most deadliest fires in the history of the state. Camp Fire is the deadliest wildfire recorded in the state of California, killing 85 people in 2018. Another fire which occurred in 2018, called the Mendocino Complex, was the second largest fire recorded in California’s history (LA Times). Both Camp Fire and Mendocino Complex are included in our dataset and analysis. It is important that our data set includes such historically significant fires such as these because it will allow us to better understand (and hopefully prevent) some of the most tragic wildfires in California history. However, in terms of visualization, because of large outliers such as the Mendocino Complex, it is harder to see the trends in the more common size fires. For this reason, in one of our preliminary data visualizations we scaled the x-axis (number of acres burned) using log base 10 so that the data is more spread out for running regressions and discovering trends.\n      It’s no secret that one of the best ways to convey information is through visualizing it. Through representing the information in our dataset visually, we hope to bring the much necessary attention to California wildfires. Although it is an intimidating thought, California will have to face many more of these deadly wildfires in its future, and with the changing climate, the outlook is not great. Through studying wildfires in the recent past, we hope to bring to light information that could help guide how fires are managed in California for the future.\n      The following code and graphics will all be completed using the data visualization software R Studios.\nWhat can we learn from geographical trends?\n\n\n\n\n\n\n\n\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"setView\":[[37.978259,-119.417931],5.5,[]],\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"http://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"http://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addMarkers\",\"args\":[39.8134,-121.4347,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"Camp Fire (2018)\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[40.65428,-122.6236,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"Carr Fire (2018)\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[37.8651,-119.5383,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"Yosemite National Park\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[38.7949,-120.3055,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"El Dorado National Forest\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[38.2353,-120.0036,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"Stanislaus National Forest\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[37.3427,-119.2244,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"Sierra National Forest\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addMarkers\",\"args\":[36.4864,-118.5658,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"Sequoia National Park\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[36.4864,40.65428],\"lng\":[-122.6236,-118.5658]}},\"evals\":[],\"jsHooks\":[]}\n      The above graph shows the geographic locations of each of the wildfires in our dataset. The smallest fires are distinguished by both the smallest points and the largest fires (determined by the number of acres of land they burned) are shown with larger points.\n      The graph highlights a few interesting findings. First, the fires with the greatest number of acres burned seem to occur in northern California. Although there are many noteworthy fires shown on the map, two in particular seem to stand out as outliers for their overwhelming number of acres burned. The largest fire, shown by the biggest yellow point, occurred in 2018 and is called the Carr Fire. The wildfire burned 229,651 acres in Redding California and killed 3 firefighters and 5 civilians. It destroyed 1,614 structures and damaged an additional 61 fires. The second largest wildfire, shown by the large, orange point and discussed above, is Camp Fire. Camp Fire, which occurred in 2018, burned 153, 336 acres in Butte County, California. At the time, Camp Fire was the deadliest and most destructive wildfire in California’s history. 85 people died as a result of Camp Fire and an additional 3 were injured by the fire. In order to contain Camp Fire, 1,065 firefighter personnel were involved. Both Camp Fire and Carr Fire are labeled on the interactive map above for reference.\n      What’s especially interesting about Camp Fire and Carr Fire both occurring in northern California is that media attention often focuses predominantly on Southern California for having a high prevalence of especially destructive fires. According to the map, fires in Southern California (surrounding Los Angeles and San Diego) all appear to be between 10,000 and 50,000 acres burned. Since the size of the fires does not reflect the amount of media attention provided to the fires, it might suggest that there is something about the fires in Southern California that garners media attention. One possible explanation could be the proximity to urban populations. The large fires in northern California occurred predominantly in expansive nature parks or reserves where there could be fewer people at risk if the fire keeps spreading. However, with the large cluster of smaller fires around Los Angeles and San Diego, there could be a lot more people and homes in harm’s way should the fire not be contained in an extremely timely manner.\n      In addition to the cluster of fires in southern California, there also appears to be a pattern in the location of the fires starting in northern California and ending east of San Jose. All of these fires seemed to have burned around 5,000 acres based on the size of the points in the map. The location of the fires seem to follow a diagonal line. The locations of the fires closely follow the locations of forest preserves and national parks on the eastern side of California. National parks such as Yosemite National Forest, Stanislaus National Forest, Sierra National Forest, and Sequoia National Park are all labeled on the interactive map above for reference.\n      Overall, the map suggests that the fires with the largest destruction occur in northern California however fires in Southern California are very worrisome for their proximity to urban populations in Los Angeles and San Diego. Further, the diagonal line of fires on the eastern side of California closely follow the locations of national parks, which can be the host of more frequent smaller fires.\n      In addition to focusing on the impact of the geographic location of the wildfires, we also want to explore how firefighting resources are utilized within California. The below plot highlights the relationship between the number of firefighting crews involved to contain the wildfire versus the number of acres burned by the fire. We chose to use a logarithmic (base 10) scale for the x-axis because without the logarithmic scale, a few extremely large fires meant that it was difficult to differentiate the smaller fires. However, since it can be difficult to convert between a log scale to understand the number of acres burned, we decided to create an interactive graph where one can hover over the point to see the number of acres burned (not using a log scale).\nHow are resources utilized within California?\n\n\n{\"x\":{\"data\":[{\"x\":[4.38472965161983,4.30732485364537,4.0580082327154,3.90703495248342,3.84849701809037,3.84292112075998,3.63808972198451,3.63808972198451,3.54468802230268,3.50051091052634,3.4929000111087,3.44420098886416,3.3912880485953,3.23248786635299,3.10414555055401,2.85247999363686,2.72754125702856,2.60852603357719,2.48429983934679,2.47421626407626,2.41995574848976,2.33645973384853,2.29225607135648,2.27875360095283,2.09691001300806,2.07918124604762,1.96378782734556,1.95904139232109,1.90308998699194,1.8750612633917,1.8750612633917,1.84509804001426,1.70757017609794,1.66275783168157,4.51075942368019,4.11902482011478,3.62736585659273,3.62736585659273,3.33102217104183,3.29994290002277,3.18977095634687,3.16702179579026,3.13830269816628,2.83058866868514,2.80071707828239,2.78675142214556,2.76342799356294,2.71264970162721,2.58994960132571,2.50514997831991,2.39269695325967,2.33645973384853,2.05307844348342,1.95424250943932,1.8750612633917,1.79239168949825,1.49136169383427,4.84159720350668,3.84509804001426,3.84509804001426,3.68868672428412,3.62838893005031,3.38845645270027,3.02077548819356,3.01786771896351,2.96378782734556,2.93449845124357,2.82801506422398,2.82607480270083,2.55022835305509,2.32633586092875,2.3096301674259,2.19865708695442,2.17609125905568,2.04139268515822,2.03742649794062,2.03342375548695,1.97772360528885,1.93449845124357,1.8750612633917,1.85733249643127,1.60205999132796,1.57978359661681,1.55630250076729,1.39794000867204,1.38021124171161,1.32221929473392,1.30102999566398,1.25527250510331,4.55959544866406,3.90902085421116,3.75823040845775,3.65069597976061,3.46893780566546,3.41077723337721,3.35044185653506,3.03342375548695,2.92941892571429,2.77742682238931,2.64443858946784,2.55630250076729,2.33243845991561,1.92941892571429,1.81291335664286,1.65321251377534,1.47712125471966,1.41497334797082,4.56295866865462,4.45768513341264,4.01464652468403,3.9645895874899,3.78168358450735,3.7805333253164,3.58815961638309,3.35583449588494,3.21722065564452,2.99782308074573,2.79934054945358,2.59328606702046,2.53019969820308,2.35218251811136,2.13033376849501,2.11394335230684,2.05690485133647,1.86923171973098,1.84509804001426,1.69897000433602,1.34242268082221,1.23044892137827,5.36106834086449,5.36106834086449,5.18564412984375,4.68948644836425,4.68948644836425,4.68948644836425,4.57987501741115,3.45984464238821,3.33485568961729,3.10209052551184,2.91645394854993,2.50514997831991,2.17609125905568,1.79934054945358,1.70757017609794,1.63346845557959,1.51851393987789,1.17609125905568,1.11394335230684,3.004751155591,2.69372694892365,1.56820172406699,4.890745081649,4.01266853389633,3.99995656838019,3.94635399722627,3.66417170536193,3.40380661054742,3.28465628278852,2.87852179550121,2.77815125038364,2.72098574415374,2.7160033436348,2.6954816764902,2.35218251811136,2.34242268082221,2.30102999566398,1.99122607569249,1.72427586960079,1.44715803134222,1.30102999566398,1],\"y\":[47,63,30,12,56,53,29,29,8,36,8,33,33,2,54,20,29,25,4,3,4,11,24,6,9,10,4,6,2,15,13,2,5,18,1,7,1,1,22,13,4,8,4,2,10,6,82,14,29,2,3,4,11,12,5,2,1,6,6,6,39,4,26,3,20,2,2,4,2,1,3,5,11,13,1,2,3,14,6,1,5,2,4,2,2,2,2,10,2,21,30,2,1,12,3,4,2,4,3,10,6,14,3,4,5,1,2,28,21,14,13,18,4,8,4,17,14,1,1,19,23,10,10,1,2,4,4,4,1,5,5,11,58,58,58,6,20,4,12,4,1,12,4,7,9,4,2,3,10,5,4,2,8,17,53,8,2,0,9,8,6,8,0,16,8,8,9,2,2,1,2],\"text\":[\" 24251\",\" 20292\",\" 11429\",\"  8073\",\"  7055\",\"  6965\",\"  4346\",\"  4346\",\"  3505\",\"  3166\",\"  3111\",\"  2781\",\"  2462\",\"  1708\",\"  1271\",\"   712\",\"   534\",\"   406\",\"   305\",\"   298\",\"   263\",\"   217\",\"   196\",\"   190\",\"   125\",\"   120\",\"    92\",\"    91\",\"    80\",\"    75\",\"    75\",\"    70\",\"    51\",\"    46\",\" 32416\",\" 13153\",\"  4240\",\"  4240\",\"  2143\",\"  1995\",\"  1548\",\"  1469\",\"  1375\",\"   677\",\"   632\",\"   612\",\"   580\",\"   516\",\"   389\",\"   320\",\"   247\",\"   217\",\"   113\",\"    90\",\"    75\",\"    62\",\"    31\",\" 69438\",\"  7000\",\"  7000\",\"  4883\",\"  4250\",\"  2446\",\"  1049\",\"  1042\",\"   920\",\"   860\",\"   673\",\"   670\",\"   355\",\"   212\",\"   204\",\"   158\",\"   150\",\"   110\",\"   109\",\"   108\",\"    95\",\"    86\",\"    75\",\"    72\",\"    40\",\"    38\",\"    36\",\"    25\",\"    24\",\"    21\",\"    20\",\"    18\",\" 36274\",\"  8110\",\"  5731\",\"  4474\",\"  2944\",\"  2575\",\"  2241\",\"  1080\",\"   850\",\"   599\",\"   441\",\"   360\",\"   215\",\"    85\",\"    65\",\"    45\",\"    30\",\"    26\",\" 36556\",\" 28687\",\" 10343\",\"  9217\",\"  6049\",\"  6033\",\"  3874\",\"  2269\",\"  1649\",\"   995\",\"   630\",\"   392\",\"   339\",\"   225\",\"   135\",\"   130\",\"   114\",\"    74\",\"    70\",\"    50\",\"    22\",\"    17\",\"229651\",\"229651\",\"153336\",\" 48920\",\" 48920\",\" 48920\",\" 38008\",\"  2883\",\"  2162\",\"  1265\",\"   825\",\"   320\",\"   150\",\"    63\",\"    51\",\"    43\",\"    33\",\"    15\",\"    13\",\"  1011\",\"   494\",\"    37\",\" 77758\",\" 10296\",\"  9999\",\"  8838\",\"  4615\",\"  2534\",\"  1926\",\"   756\",\"   600\",\"   526\",\"   520\",\"   496\",\"   225\",\"   220\",\"   200\",\"    98\",\"    53\",\"    28\",\"    20\",\"    10\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,1.0552033967198,1.11040679343961,1.16561019015941,1.22081358687921,1.27601698359902,1.33122038031882,1.38642377703863,1.44162717375843,1.49683057047823,1.55203396719804,1.60723736391784,1.66244076063764,1.71764415735745,1.77284755407725,1.82805095079705,1.88325434751686,1.93845774423666,1.99366114095647,2.04886453767627,2.10406793439607,2.15927133111588,2.21447472783568,2.26967812455548,2.32488152127529,2.38008491799509,2.43528831471489,2.4904917114347,2.5456951081545,2.6008985048743,2.65610190159411,2.71130529831391,2.76650869503372,2.82171209175352,2.87691548847332,2.93211888519313,2.98732228191293,3.04252567863273,3.09772907535254,3.15293247207234,3.20813586879215,3.26333926551195,3.31854266223175,3.37374605895156,3.42894945567136,3.48415285239116,3.53935624911097,3.59455964583077,3.64976304255057,3.70496643927038,3.76016983599018,3.81537323270999,3.87057662942979,3.92578002614959,3.9809834228694,4.0361868195892,4.091390216309,4.14659361302881,4.20179700974861,4.25700040646841,4.31220380318822,4.36740719990802,4.42261059662783,4.47781399334763,4.53301739006743,4.58822078678724,4.64342418350704,4.69862758022684,4.75383097694665,4.80903437366645,4.86423777038625,4.91944116710606,4.97464456382586,5.02984796054566,5.08505135726547,5.14025475398527,5.19545815070508,5.25066154742488,5.30586494414468,5.36106834086449],\"y\":[0.648632087110212,1.02020790116609,1.38908137463516,1.75557254528455,2.1200014508814,2.48268812919285,2.84395261798602,3.20411495502804,3.56349517808605,3.92241332492719,4.28118943331857,4.64014354102734,4.99968558007045,5.36326489417578,5.7296029102771,6.09468924546492,6.45451351682974,6.80506534146206,7.15208240125403,7.52386208040513,7.91083082214744,8.29975535774548,8.67740241846378,9.03053873556689,9.34593104031935,9.61148460927021,9.83062136088774,10.0166280332753,10.1829064458631,10.3428584180813,10.5098857693603,10.6973903191301,10.8484727638682,10.9053630595287,10.9231760512226,10.9576860385402,11.0646673210715,11.297701785122,11.5982195731958,11.9208577722282,12.2705942431405,12.6524068468542,13.0712734442906,13.5321718963711,14.0400800640171,14.5999758081499,15.2327412744019,15.9510592462194,16.7239282749739,17.5196295065003,18.3064440866334,19.0526531612083,19.7265378760597,20.2963793770225,20.751092538261,21.149933048951,21.4995004184753,21.7998464313671,22.0510228721596,22.2530815253861,22.4060741755797,22.5100526072736,22.565068605001,22.5711739532952,22.5284204366894,22.4368598397167,22.2965439469103,22.1075245428036,21.8698534119295,21.5835823388215,21.2487631080126,20.8654475040361,20.4336873114252,19.9535343147131,19.425040298433,18.8482570471181,18.2232363453015,17.5500299775166,16.8286897282965,16.0592673821745],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"name\":\"fitted values\",\"line\":{\"width\":3.77952755905512,\"color\":\"rgba(51,102,255,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":47.0004151100042,\"r\":7.30593607305936,\"b\":44.2175176421752,\"l\":39.6513075965131},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":15.9402241594022},\"title\":{\"text\":\"Crews Involved v. Log of Acres Burned\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":19.1282689912827},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.781946582956776,5.57912175790771],\"tickmode\":\"array\",\"ticktext\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"tickvals\":[1,2,3,4,5],\"categoryorder\":\"array\",\"categoryarray\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"serif\",\"size\":12.7521793275218},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Log of Acres Burned\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":15.9402241594022}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-4.1,86.1],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"tickvals\":[0,20,40,60,80],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"serif\",\"size\":12.7521793275218},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"Crews Involved\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":15.9402241594022}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,\"borderwidth\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":12.7521793275218}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"94b157d1803\":{\"text\":{},\"x\":{},\"y\":{},\"type\":\"scatter\"},\"94b47b377fa\":{\"x\":{},\"y\":{}}},\"cur_data\":\"94b157d1803\",\"visdat\":{\"94b157d1803\":[\"function (y) \",\"x\"],\"94b47b377fa\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\n      The graph suggests that wildfires which burned less than 100 acres (labeled between 1 and 2 on the x-axis) required fewer than 20 firefighting crews to contain the fire. Similarly, fires which burned between 100 and 1,000 acres (shown between 2 and 3 on the x-axis) could be contained with less than 30 firefighting crews. Interestingly, there seems to be an outlier fire, which burned 580 acres but required over 80 firefighting crews. There also seems to be a sharp increase in the number of crews needed to contain fires which burn between 3,000 and 50,000 acres (between 3.5 and 4.5 on the x-axis).\n      To further understand the trends in the number of crews needed to fight the fires, we added a Loess regression line to the data. The Loess regression is able to fit a smooth curve across the data points and is beneficial for understanding patterns that a linear regression could miss. What’s most intriguing about the Loess regression line within the graph is that it appears to increase at a nearly constant rate between 1 and 3 (or between 10 and 10,000 acres). However as wildfires burn over 10,000 acres there seems to be a steeper increase in the number of crews required.\n      The regression line seems to suggest that fewer crews were provided to fight the two largest fires, which burned 153,336 and 229,651 acres. While we have analysed the geographic importance of these 2 fires above, this graph seems to suggest that there were not enough crews available to assist in fighting the fires. Possible explanations for the small number of crews fighting the largest fires could have to do with the timing of the fires. One possible explanation for this phenomenon is that the largest fire, which we will call the Carr Fire, began on July 23, 2018 and lasted for over 164 days. This means that Carr Fire was not extinguished until January 4, 2019. The second largest fire, known as Camp Fire began on November 8,2018 and lasted for 17 days. This means that there was an overlapping period where resources had to be split between the two fires. Further, since the Carr Fire lasted for over 164 days, there were probably many other fires occurring at the same time, which could have made it difficult to deploy enough crews to contain the fire in a timely manner.\nHow does the duration of the fires change year-to-year?\n\n\n{\"x\":{\"data\":[{\"x\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"y\":[8.97847222222222,5.16319444444444,6.10416666666667,5.79027777777778,8.72222222222222,7.99166666666667,6.28819444444444,6.28819444444444,5.67430555555556,3.24444444444444,6.18194444444444,3.22569444444444,6.05763888888889,9.87708333333333,4.75763888888889,3.28125,2.64583333333333,4.48888888888889,1.18263888888889,0.257638888888889,3.2125,3.95833333333333,4.06041666666667,1.86875,2.17638888888889,2.2,2.94722222222222,1.16875,1.02569444444444,1.99305555555556,1.78125,2.02291666666667,1.27083333333333,1.87430555555556],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(248,118,109,0.55)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"(2013,1)\",\"legendgroup\":\"(2013,1)\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],\"y\":[24.6638888888889,13.4993055555556,8.11180555555556,8.11180555555556,5.39583333333333,8.09375,4.38472222222222,3.63472222222222,6.375,7.12291666666667,5.80555555555556,6.77430555555556,4.1,26.2166666666667,4.17986111111111,7.73541666666667,5.19305555555556,2.93888888888889,2.13958333333333,2.09444444444444,2.99027777777778,1.01180555555556,1.06111111111111],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(196,154,0,0.55)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"(2014,1)\",\"legendgroup\":\"(2014,1)\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3],\"y\":[16.1152777777778,5.72777777777778,5.72777777777778,20.1340277777778,3.68541666666667,12.0013888888889,5.55416666666667,6.21875,6.90625,0.892361111111111,6.18819444444444,4.08333333333333,0.193055555555556,3.04513888888889,6.0625,3.94305555555556,2.41527777777778,0.0958333333333333,1.79652777777778,7.24444444444444,4.76319444444444,6.21180555555556,2.09027777777778,1.26180555555556,0.141666666666667,0.946527777777778,0.765277777777778,1.87083333333333,1,0.247222222222222,1.43958333333333,1.16388888888889],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(83,180,0,0.55)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"(2015,1)\",\"legendgroup\":\"(2015,1)\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"y\":[6.87569444444444,8.82083333333333,9.93333333333333,357.825,3.57569444444444,10.8104166666667,10.0763888888889,8.40277777777778,6.93263888888889,12.6194444444444,3.34375,-0.361111111111111,2.16666666666667,2.19097222222222,2.07430555555556,2.18958333333333,0.126388888888889,0.845138888888889],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(0,192,148,0.55)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"(2016,1)\",\"legendgroup\":\"(2016,1)\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5],\"y\":[133.179166666667,186.834722222222,235.870138888889,178.038888888889,43.9354166666667,185.871527777778,128.970138888889,186.961111111111,175.946527777778,119.265972222222,194.80625,131.881944444444,215.939583333333,190.903472222222,209.730555555556,130.354166666667,150.984722222222,184.8,175.945138888889,119.735416666667,210.009027777778,144.935416666667],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(0,182,235,0.55)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"(2017,1)\",\"legendgroup\":\"(2017,1)\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\"y\":[164.848611111111,164.848611111111,17.0604166666667,160.845138888889,160.845138888889,160.845138888889,182.891666666667,179.845833333333,146.682638888889,259.111805555556,181.8,165.854861111111,121.754166666667,167.841666666667,163.758333333333,168.902777777778,102.347222222222,263.213888888889,167.855555555556],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(165,138,255,0.55)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"(2018,1)\",\"legendgroup\":\"(2018,1)\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7],\"y\":[null,3.30902777777778,2.12439814814815,13.8979166666667,76.142650462963,5.87606481481482,8.23615740740741,null,11.1567476851852,10.0947222222222,null,4.24850694444444,1.08136574074074,null,6.1625,5.04513888888889,6.14861111111111,null,0.984722222222222,0.708333333333333,0.734027777777778,0.249305555555556,0.157164351851852],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(251,97,215,0.55)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"(2019,1)\",\"legendgroup\":\"(2019,1)\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[0.642075458168983,0.8322200210765,1.30008824188262,0.787884734198451,0.92290385812521,0.633231240510941,0.838916376233101,0.956630400009453,0.729535315744579,1.29179515521973,0.988974606432021,1.02134095709771,1.21015492938459,0.690511777065694,0.714444035850465,0.854132950864732,0.788711276091635,1.21556070223451,0.740282986313105,1.08073584586382,1.36144951079041,0.977203256078064,0.724332563020289,0.626024384237826,1.08867387119681,1.33477597683668,0.797725326754153,0.861078451015055,1.04109164252877,0.979564731195569,1.08896597828716,1.1457721915096,1.25302675273269,0.969494347088039],\"y\":[8.97844281301342,5.16357029185941,6.10363137426683,5.79009563289971,8.72210979609957,7.99147989705774,6.28783304011553,6.28839871422,5.67405650997302,3.24408138044411,6.18237929271379,3.22523458584703,6.05771521558261,9.87743854610896,4.75799950628878,3.28119002872561,2.64580111226714,4.48918718244855,1.18303874650892,0.257809316076907,3.21291765152632,3.95813267926142,4.06083348722068,1.86880430976886,2.17641649835148,2.20052323186718,2.94746700928728,1.16922484873458,1.02554172191355,1.99291208705037,1.78087806751693,2.02301381496713,1.27123845988315,1.87425280330573],\"text\":[\" 24251\",\" 20292\",\" 11429\",\"  8073\",\"  7055\",\"  6965\",\"  4346\",\"  4346\",\"  3505\",\"  3166\",\"  3111\",\"  2781\",\"  2462\",\"  1708\",\"  1271\",\"   712\",\"   534\",\"   406\",\"   305\",\"   298\",\"   263\",\"   217\",\"   196\",\"   190\",\"   125\",\"   120\",\"    92\",\"    91\",\"    80\",\"    75\",\"    75\",\"    70\",\"    51\",\"    46\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\"}},\"hoveron\":\"points\",\"name\":\"(2013,1)\",\"legendgroup\":\"(2013,1)\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[2.15436032246798,2.142530589737,2.33111324384809,2.3440754307434,1.78861141242087,1.6338540604338,2.12820339854807,1.67430633604527,2.26492909993976,1.72142129410058,1.81199386734515,1.76882627420127,2.20971292182803,1.82011638097465,2.28854952771217,2.26790144499391,1.86259844135493,1.9699098451063,2.24741326998919,2.31188326068223,1.62030564621091,1.63724005147815,2.2243223529309],\"y\":[24.663628187988,13.4994316739354,8.11132095207615,8.11227844635309,5.39578858815392,8.09341726262459,4.38498188527534,3.63450377758618,6.37504889259218,7.12304515585795,5.80518823505457,6.77384207384687,4.09946980636412,26.2166605965022,4.17970935183112,7.73495013664502,5.19279752903773,2.93929920583291,2.13926504435199,2.094687443938,2.9907081314448,1.01190237226482,1.06061244679822],\"text\":[\" 32416\",\" 13153\",\"  4240\",\"  4240\",\"  2143\",\"  1995\",\"  1548\",\"  1469\",\"  1375\",\"   677\",\"   632\",\"   612\",\"   580\",\"   516\",\"   389\",\"   320\",\"   247\",\"   217\",\"   113\",\"    90\",\"    75\",\"    62\",\"    31\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(196,154,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(196,154,0,1)\"}},\"hoveron\":\"points\",\"name\":\"(2014,1)\",\"legendgroup\":\"(2014,1)\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[2.99033445231616,2.69760126695037,2.76858630012721,3.00640248432755,2.83032606653869,2.82891165167093,3.08790227361023,2.82562581337988,3.26619172673672,2.92500111758709,3.06343398001045,3.2131064735353,2.83649006057531,2.61629735082388,2.74248796887696,3.12075474914163,3.27291738204658,2.9408277336508,3.08034050744027,3.37605237290263,2.98176462464035,3.10654145106673,2.90085110608488,2.99010895881802,2.80163105688989,2.6366375759244,2.6958205608651,2.69128697831184,3.28683926165104,3.04035897236317,2.63306218124926,2.82523140255362],\"y\":[16.1151424315233,5.72790075578416,5.72795208412688,20.1343891956163,3.68586943689682,12.0019023414313,5.55468699884653,6.2190922107607,6.90622768952408,0.892101370208224,6.18789868914781,4.08324312611509,0.192813199019163,3.04499983766815,6.06243561452462,3.94268523354445,2.41561115670147,0.0963510200927138,1.79601734830558,7.24457306458822,4.76277144092575,6.21176541212274,2.09018768464551,1.26137155605713,0.14199770921298,0.946640584343372,0.765220147869032,1.87065756642544,0.999778532309509,0.247229172825141,1.43936686271905,1.16378938498001],\"text\":[\" 69438\",\"  7000\",\"  7000\",\"  4883\",\"  4250\",\"  2446\",\"  1049\",\"  1042\",\"   920\",\"   860\",\"   673\",\"   670\",\"   355\",\"   212\",\"   204\",\"   158\",\"   150\",\"   110\",\"   109\",\"   108\",\"    95\",\"    86\",\"    75\",\"    72\",\"    40\",\"    38\",\"    36\",\"    25\",\"    24\",\"    21\",\"    20\",\"    18\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(83,180,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(83,180,0,1)\"}},\"hoveron\":\"points\",\"name\":\"(2015,1)\",\"legendgroup\":\"(2015,1)\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[3.71412062384188,3.65050321035087,4.04849406182766,3.95533984396607,3.98706161845475,3.88506412804127,4.26843540892005,4.13848760370165,4.20068534724414,3.66277177724987,3.66276685446501,3.75881279334426,3.67855275068432,4.08466490712017,4.06970526371151,3.88990377560258,4.10721057765186,3.82859735935926],\"y\":[6.87533053306242,8.82119784708704,9.93378685052144,357.824788201178,3.57574139142719,10.8104268407654,10.0765016067815,8.40265418050578,6.93259535871058,12.619176510002,3.3439467655606,-0.361283217658185,2.16709772495191,2.19151625521093,2.07390851150965,2.19010553171781,0.126860766354172,0.845085482370471],\"text\":[\" 36274\",\"  8110\",\"  5731\",\"  4474\",\"  2944\",\"  2575\",\"  2241\",\"  1080\",\"   850\",\"   599\",\"   441\",\"   360\",\"   215\",\"    85\",\"    65\",\"    45\",\"    30\",\"    26\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,192,148,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,192,148,1)\"}},\"hoveron\":\"points\",\"name\":\"(2016,1)\",\"legendgroup\":\"(2016,1)\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[5.10051585640758,5.31311909314245,4.86538822315633,4.63598681390286,5.17890252675861,4.86433988548815,5.33665283229202,5.14148371741176,5.14682859797031,4.92704229447991,4.93459869567305,5.28454268295318,4.66653057001531,5.1801087718457,5.23126905225217,4.91063791085035,4.63998866938055,5.32964583728462,4.83960778173059,5.05499373953789,5.09798529241234,5.0655664306134],\"y\":[133.17943515511,186.834731297428,235.870655357249,178.039436553822,43.9357287693308,185.871129901096,128.970337248891,186.961505379521,175.946609688708,119.265820582918,194.806329722426,131.882214681153,215.93934675474,190.903586780063,209.730423217807,130.354216685242,150.984740073782,184.800279856803,175.945595700429,119.735528950742,210.008934704494,144.935463704704],\"text\":[\" 36556\",\" 28687\",\" 10343\",\"  9217\",\"  6049\",\"  6033\",\"  3874\",\"  2269\",\"  1649\",\"   995\",\"   630\",\"   392\",\"   339\",\"   225\",\"   135\",\"   130\",\"   114\",\"    74\",\"    70\",\"    50\",\"    22\",\"    17\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,182,235,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,182,235,1)\"}},\"hoveron\":\"points\",\"name\":\"(2017,1)\",\"legendgroup\":\"(2017,1)\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[5.62474604826421,5.95418905355036,6.30765482317656,6.34387507755309,6.04377416670322,6.21448915563524,6.00432045739144,5.7731085292995,5.94472601749003,5.86357840094715,5.66740977540612,5.80386301409453,5.66804139036685,5.90108878724277,6.16666944865137,5.89864852782339,6.32969146221876,6.11551268845797,5.80925829242915],\"y\":[164.848523312909,164.848825759228,17.0606719320624,160.845020467047,160.845338459866,160.845014143405,182.891788739859,179.845929110806,146.682421341039,259.111818187951,181.799956624005,165.855064188443,121.753921223751,167.841875480767,163.758013458614,168.902958330516,102.346766542045,263.214388316637,167.855418452797],\"text\":[\"229651\",\"229651\",\"153336\",\" 48920\",\" 48920\",\" 48920\",\" 38008\",\"  2883\",\"  2162\",\"  1265\",\"   825\",\"   320\",\"   150\",\"    63\",\"    51\",\"    43\",\"    33\",\"    15\",\"    13\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(165,138,255,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(165,138,255,1)\"}},\"hoveron\":\"points\",\"name\":\"(2018,1)\",\"legendgroup\":\"(2018,1)\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[7.0003750147298,6.65973991192877,6.97980554960668,7.38407772164792,6.8092661164701,7.1642182039097,6.77583172339946,7.03947429750115,6.93297922238708,6.85873173903674,7.20433570947498,6.66873259376734,7.25897131189704,6.85129331741482,7.00688507463783,6.85711190905422,6.92589370459318,6.99309058599174,6.70278328750283,6.63948142919689,6.97394713498652,7.26877176146954,6.75394705981016],\"y\":[null,3.30956838235595,2.12440572459492,13.8976907107951,76.1425330271188,5.87657902678354,8.23582532705446,null,11.1570622393949,10.0943397521122,null,4.24837079670746,1.08124302357918,null,6.16196664818433,5.04470877637181,6.14872932890249,null,0.985141309490686,0.708337735425577,0.733677188307047,0.249657179554666,0.156768582374961],\"text\":[\"  1011\",\"   494\",\"    37\",\" 77758\",\" 10296\",\"  9999\",\"  8838\",\"  4615\",\"  2534\",\"  1926\",\"   756\",\"   600\",\"   526\",\"   520\",\"   496\",\"   225\",\"   220\",\"   200\",\"    98\",\"    53\",\"    28\",\"    20\",\"    10\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(251,97,215,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(251,97,215,1)\"}},\"hoveron\":\"points\",\"name\":\"(2019,1)\",\"legendgroup\":\"(2019,1)\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":47.0004151100042,\"r\":7.30593607305936,\"b\":28.2772934827729,\"l\":46.027397260274},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":15.9402241594022},\"title\":{\"text\":\"Calfornia Wildfire Duration by Year\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":19.1282689912827},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,7.6],\"tickmode\":\"array\",\"ticktext\":[\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\"],\"tickvals\":[1,2,3,4,5,6,7],\"categoryorder\":\"array\",\"categoryarray\":[\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"serif\",\"size\":12.7521793275218},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":15.9402241594022}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-18.2705973785411,375.734314160883],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"100\",\"200\",\"300\"],\"tickvals\":[0,100,200,300],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"100\",\"200\",\"300\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"serif\",\"size\":12.7521793275218},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"Fire Duration (in days)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":15.9402241594022}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,\"borderwidth\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"serif\",\"size\":12.7521793275218}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"94b38f3d430\":{\"fill\":{},\"alpha\":{},\"x\":{},\"y\":{},\"type\":\"box\"},\"94bfefb886\":{\"colour\":{},\"text\":{},\"x\":{},\"y\":{}}},\"cur_data\":\"94b38f3d430\",\"visdat\":{\"94b38f3d430\":[\"function (y) \",\"x\"],\"94bfefb886\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\n      Now that we understand where the fires took place and how resources were being used to help, we wanted to explore how the duration of forest fires has changed over time. We chose to use a box-plot to represent this data because we believe it gives the best visualization of how long the majority of fires lasted, from the 25th to 75th percentiles, while also showing the outliers. The graph above specifically represents the range of fire durations in each given year from 2013 to 2019. We also chose to represent each fire with a dot over the box-plot in order to show how many fires the plots are accounting for. Further, by interacting with each point on the plot, one can see the number of acres burned by each fire. This way, one can also see the relationship between fire duration and acres burned by each wildfire.\n      The graph above suggests that the duration of fires from 2013 to 2016 stayed pretty consistent, with all the fires lasting for less than 30 days. The one exception to this trend is a 2016 wildfire which lasted over 300 days and burned 4,474 acres in Fresno . This Fresno fire is denoted on the boxplot by the green dot at the top of the boxplot for 2016. It is interesting that the fire in Fresno had such a significantly longer duration, when it only burned half as many acres as the largest fire that year.\n      It can also be seen from the graph that in 2017 and 2018 there was a large increase in the duration of most wildfires. What makes 2017 particularly interesting is that the fire duration increases so drastically from the earlier years but from 2013 to 2017 there was not a large increase in the amount of acres burned by the fires. However, fires from 2018 tell a completely different story. The wildfires in 2018 went down in history as some of the worst fires recorded in California’s history. With that being said, the inner-quartile range for fires in 2018 is much smaller than 2017. This suggests that there were a few historically large fires from 2018 but that the majority of the 2018 fires lasted between 150 and 190 days. In 2018 it seems to be that there were a few outliers in terms of extremely long fires but also outliers on the lower end of the spectrum in terms of their duration.\n      Another interesting finding that can be gathered from the graph is that 2017 had the greatest amount of variation in terms of fire length. 2017 has the largest range for fire durations, with fires that were as short as 50 days and others as long as 230 days.The dots show us that the fires during 2017 were not very consistent, but varied quite greatly causing the large interquartile range that is seen on the plot. In contrast to this, 2018 had a slightly lower median duration length. However, although the median was smaller, the interquartile range is much smaller showing that the fires were consistently pretty bad during that time period. Therefore, although 2017 has a higher median and a large range, 2018 has a larger proportion of fires with consistent long durations, which is why it was recorded as a much more severe year than the previous.\n      It can also be seen from this graph that 2019 did not have many fires with long durations, and in fact followed more closely to the trends seen between 2013 and 2016. While we cannot say for sure the reasoning behind the longer fire durations in 2017 and 2018, possible explanations could range from environmental factors such as the amount of rain from earlier in the year to the geographical locations of the fires or to the amount of resources that could be provided for the fires. What we can say for sure though is that there must have been something special about the conditions of the forests or the weather during 2017 and 2018 that made the fires that much worse than in the past.\nHow does the number of crews involved with the fires change year-to-year?\n\n\n\n      After taking a deep dive into the different durations of fires between 2013 and 2019, we wanted to see how the year-by-year changes in fire durations related to the number of crews that were involved fighting the fires. To best highlight the changes between each year, we decided to animate the graph and change the color of the points to represent different years.\n      The graph suggests that in 2013, while the fires did not last long, there was a large spread in the number of crews that were required in order to contain the fires. The number of crews required in 2013 ranges from just a few crews to over 60, even though fire duration does not exceed 25 days. This suggests that in 2013, the duration of the fires was not a predictor for the number of firefighters needed to extinguish the fires.\n      Then in 2014, the maximum fire duration is slightly higher than in 2013 but the majority of fires are still relatively short. All of the fires in 2014 lasted fewer than 50 days and required fewer than 30 crews except for a single outlier that required over 80 crews. Then, 2015 and 2016 follow a similar pattern to 2014. The majority of fires are short in their duration and do not require a large number of crews. All fires in 2015 required fewer than 40 crews and lasted fewer than 2015 days. Similarly, in 2016 (except for the outlier lasting over 300 days discussed above), all fires lasted fewer than 20 days and required no more than 40 crews. This suggests that the environmental factors and resources required to fight the fires between 2104 and 2016 were very similar.\n      What’s particularly interesting about the Fresno fire in 2016 which lasted over 300 days is that there were very few crews allocated to the fire. This could mean that since the fire lasted so long, resources continuously needed to be diverted to other fires which could be contained more quickly.\n      As predicted by the boxplot above, the durations of the fires in 2017 and 2018 were larger than the previous years. However, it’s interesting that even with their long duration, all of the fires in 2017 required fewer than 30 crews. 2018 follows a similar pattern to 2017. Regardless of fire duration, all but one fire in 2018, required fewer than 20 crews. The trends in 2017 and 2018 further support the notion that fire duration cannot predict the number of crews either required or allocated to fight the fires. One possible explanation for this is that firefighting resources are finite and thus there are not enough crews to be able to continually support large fires. It could be the case that firefighters prioritize fires by their location or it could be that the number of simultaneous fires impacts the amount of resources provided to each wildfire.\nWhat do the changes in the number of injuries, personnel involved, and structures damaged tell us about California wildfires?\n\n\n\n      Since we know now how fire duration varied over the years, we wanted to investigate the way the number of injuries, personnel involved, and structures damaged also varied over time. We chose to graph this data with bar charts since the values we are representing are counts. The y-axis across all three plots stays consistent in order to provide direct comparison between the three variables displayed on the x axises.\n      Beginning with the first plot, “Number of Injuries by Year”, we can see that the number of injuries over time has been following an overall decreasing rate, starting from 2013 to 2019. This is very interesting because as we can see in the third plot, the number of structures damaged had an overall increasing pattern. There is no way to know from the data, but this leads us to believe that because more time and resources went to helping people, more structures were damaged as more injuries were avoided.\n      Taking a deeper look into the third plot, “Structures Damaged by Year”, the number of structures damaged was consistently low from 2013-2016 but had a sudden massive increase in 2017 and has stayed relatively high ever since. The data shown in 2017 and 2018 relates back to the long duration times which were shown in the previous graph above. One can be led to believe that because the fires lasted longer, more structures were damaged as a result. However, 2019 did not burn a large amount of acres nor have long duration times which is why it is very intriguing that there were still lots of structures damaged during that time period.\n      Plot two, “Number of Personnel Involved by Year”, was the most predictable out of the three plots based on our past analysis. The time period from 2014-2016 did not have many significant fires, which is why a large amount of personnel was not required to contain them. With the rise of fires in 2017 and 2018, it is sensible that the amount of personnel involved had to increase in order to combat them. The only peculiar part of this graph is the large amount of personnel that was called upon in 2013, for it surpasses 2018 which had a significantly greater amount of acres burned. Although there is no way to confirm this with the data, our hypothesis is that over time as technology advanced, less attendants were needed to contain the fires which is the reason for the sudden drop in personnel. However we now know that the forest fire season in California has only been worsening, and as it can be seen in plot three, the damages are not diminishing, therefore it is evident that although technology has improved, the need for personnel is still very much present.\nDiscussion:\n      As we wrap up our project we want to highlight a few limitations to the conclusions we could draw from our analysis. First and foremost our conclusions were limited by our lack of formal knowledge about the environmental factors that cause wildfires. Many of the outliers in our visualizations could possibly be explained scientifically, such as abnormal fire duration length, however, we do not have the vast knowledge to make those claims.\n      Further, the number of variables we could explore in our analysis was heavily limited by missing data. Many factors we wanted to explore such as fatalities, number of fire engines / water tenders used, as well as the number of structures damaged by the fires had a multitude of missing values so we could not conduct thorough analysis or create data visualizations with these variables. Unlike what one might suspect, we could not assume that a value of “NA” corresponded to 0 because there could have been unique factors about the wildfires with missing values. For example, it could be that whoever recorded the data for specific wildfires chose not to report certain variables (ex. fatalities). This could mean that a value of “NA” for a variable such as fatalities just means the count was not reported, not that a wildfire did not cause any fatalities. For this reason, we chose not to analyze variables with an extremely large amount of missing data but in the future we would love to conduct more robust analysis if the data is available.\n      In future analysis we also would have loved to analyze wildfires which happened in 2020. As we have discussed, wildfires in 2020 broke records in California for the vast amount of destruction they caused. When it becomes available we would love to conduct further analysis including 2020 wildfire data in order to better understand possible trends and causes for such a dangerous year.\n      In addition to including 2020 data, it would have been helpful if our dataset had included unique names for each fire. While we were able to look up names for some of the largest fires in our dataset, it would have been helpful to have some kind of unique identifier to draw further comparisons. These comparisons could include looking at the top 10 largest, most deadliest, or even longest-lasting fires.\n      Finally, it would also be useful to have data on the smoke produced by each fire. This data would allow us to better understand the social and health impact of the wildfires in California. Over the past few years, many California residents have had to worry about smoke inhalation due to the immense amount produced in the air due to the wildfires. Further, smoke can travel a great distance not only throughout the state of California but also nearby states. Thus, it would be extremely relevant and significant to better understand the factors and predictors of immense smoke in future analysis.\n      We hope you have enjoyed learning more about California wildfires through our project and we look forward to conducting further research on such an important topic.\nSources:\nhttps://www.theguardian.com/us-news/2020/dec/30/california-wildfires-north-complex-record\nhttps://www.latimes.com/california/story/2020-09-11/six-of-californias-largest-fires-in-history-are-burning-right-now\nAppendix: Code\n\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(ggthemes)\n\nfinal_project_theme = theme_minimal() + \n  theme(text = element_text(family = \"serif\", size = 12),\n        axis.title = element_text(),\n        plot.title.position = \"plot\")\nfires <- read_csv(\"CaliforniaFires.csv\")\n\n# Map of Wildfires\nlibrary(ggmap)\n\nca <- c(left = -129, bottom = 32, right = -114, top = 42.3) \nmap <- get_stamenmap(ca, zoom = 5, maptype = \"toner-lite\") \n\nggmap(map) +\n  geom_point(data = fires, aes(x = Longitude, y = Latitude, size = AcresBurned), \n             alpha = 0.5, color = \"orange red\") +\n  labs(\n    title = \"Locations of California Wildfires\",\n    size = \"Acres Burned (in thousands)\") +\n  scale_size_area(breaks = 1000 * c(5, 10, 50, 100, 150, 200),\n             labels = c(\"5\", \"10\",\"50\", \"100\",\"150\",\"200\")) +\n  theme_void() +\n  theme(text = element_text(family = \"serif\"))\n\nfires_sorted_acres_burned <- fires[order(fires$AcresBurned),]\n#Create leaflet map\nlibrary(leaflet)\n\nca_leaflet_map = leaflet() %>% \n  setView(lat = 37.978259, lng = -119.417931, zoom = 5.5) %>%\n  addTiles() %>%\n  addMarkers(lat = 39.81340, lng =-121.4347  , popup = \"Camp Fire (2018)\") %>% \n  addMarkers(lat = 40.65428, lng =-122.6236  , popup = \"Carr Fire (2018)\")%>% \n  addMarkers(lat = 37.8651, lng =-119.5383  , popup = \"Yosemite National Park\") %>% \n  addMarkers(lat = 38.7949, lng =-120.3055  , popup = \"El Dorado National Forest\")%>% \n  addMarkers(lat = 38.2353, lng =-120.0036  , popup = \"Stanislaus National Forest\")%>% \n  addMarkers(lat = 37.3427, lng =-119.2244  , popup = \"Sierra National Forest\")%>% \n  addMarkers(lat = 36.4864, lng =-118.5658  , popup = \"Sequoia National Park\")\n\nca_leaflet_map\n\n#Acres Burned v. Crews Involved Scatterplot\np0 <- ggplot(data = fires, aes(x = log10(AcresBurned), y = CrewsInvolved)) +\n  geom_point(aes(text = AcresBurned)) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  final_project_theme +\n  labs(\n    title = \"Crews Involved v. Log of Acres Burned\", \n    x = \"Log of Acres Burned\",\n    y = \"Crews Involved\") \n\nggplotly(p0, tooltip = \"text\")\n#Calculating Fire Duration & Creating Boxplot\nfires$fire_duration = difftime(fires$Extinguished,fires$Started,units='days')\n\np1 <- ggplot(data = fires, aes(x = as.factor(ArchiveYear), group = ArchiveYear, \n                               y = fire_duration)) + \n        geom_boxplot(aes(fill = as.factor(ArchiveYear), alpha = 0.3)) +\n        geom_jitter(aes(color = as.factor(ArchiveYear), text = AcresBurned)) +\n        theme_minimal() +\n        labs(\n          title = \"Calfornia Wildfire Duration by Year\",\n          x = \"\",\n          y = \"Fire Duration (in days)\") +\n        final_project_theme +\n        theme(legend.position = \"none\") \n\nggplotly(p1, tooltip = \"text\")\n\n#Create animated scatterplot Crews Involved v. Fire Duration\nlibrary(gganimate)\nlibrary(png)\nlibrary(gifski)\nlibrary(glue)\n\nanim1 = ggplot(data = fires, aes(x = fire_duration, y = CrewsInvolved)) +\n  geom_point(aes(color = as.factor(ArchiveYear)))+\n  final_project_theme +\n  labs(x = \"Fire Duration (in days)\", y = \"Crews Involved\", col = \"Year\")+\n  theme(legend.position = \"right\",\n        legend.direction='vertical') +\n  transition_states(as.factor(ArchiveYear), transition_length = 5,\n                    state_length = 10) +\n  enter_fade() +\n  exit_shrink() +\n  ggtitle(\"Crews Involved v. Fire Duration by Year\",\n          subtitle = \"Now showing {closest_state}\")\n\nanimate(anim1, nframes = 100, duration = 21)\n#Create 3 barcharts\nlibrary(gridExtra)\n\nb1 <- ggplot(fires, aes(x = Injuries, y = as.factor(-ArchiveYear))) + \n  geom_col(aes(fill = as.factor(ArchiveYear)))+\n  final_project_theme +\n  theme(legend.position = \"none\") +\n  labs(y = \"\", x = \"Injuries\") +\n  scale_fill_brewer(palette = \"YlOrRd\")\n  \n\nb2 <- ggplot(fires, aes(x = StructuresDamaged, y = as.factor(-ArchiveYear))) + \n   geom_col(aes(fill = as.factor(ArchiveYear)))+\n  final_project_theme + \n  theme(legend.position = \"none\") +\n  labs(y = \"\", x = \"Structures Damaged\") +\n  scale_fill_brewer(palette = \"YlOrRd\")\n\n\nb3 <- ggplot(fires, aes(x = PersonnelInvolved, y = (as.factor(-ArchiveYear)))) + \n   geom_col(aes(fill = as.factor(ArchiveYear)))+\n  final_project_theme +\n  theme(legend.position = \"none\") +\n  labs(y = \"\", x = \"Personnel Involved\") +\n  scale_fill_brewer(palette = \"YlOrRd\")\n\n# https://forcats.tidyverse.org/reference/fct_reorder.html\n\ngrid.arrange(b1, b3, b2, ncol=3)\n\nggplot(fires, aes(x = PersonnelInvolved, y = (as.factor(-ArchiveYear)))) + \n   geom_col(aes(fill = as.factor(ArchiveYear)))+\n  final_project_theme +\n  theme(legend.position = \"none\") +\n  labs(y = \"\", x = \"Personnel Involved\") +\n  scale_fill_brewer(palette = \"YlOrRd\")\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-26-california-forest-fires/california-forest-fires_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-26T13:06:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Swat Data Viz",
    "description": "Welcome to the new site! I hope it fills out with some beautiful visualizations soon.",
    "author": [
      {
        "name": "Amanda Luby",
        "url": "https://swarthmore.edu/NatSci/aluby1"
      }
    ],
    "date": "2021-01-25",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-25T19:54:09-05:00",
    "input_file": {}
  }
]
